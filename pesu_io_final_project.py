# -*- coding: utf-8 -*-
"""PESU/IO FINAL PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_atIZyN4Lsc796c3XbUrX66eXIJR4xZG
"""

!pip install openai
!pip install tiktoken
!pip install langchain
!pip install faiss-cpu

pip install pypdf

import openai
import os
os.environ['OPENAI_API_KEY']='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'
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import LLMChain
from langchain.chains.question_answering import load_qa_chain
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.prompts import SystemMessagePromptTemplate
from langchain import PromptTemplate
from langchain.memory import ConversationBufferMemory

from langchain.document_loaders import PyPDFLoader
loader = PyPDFLoader("/content/SchaeferSedlmeier2009.pdf")
pages  = loader.load_and_split()

from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size = 800, chunk_overlap=50)
documents = text_splitter.split_documents(pages)

embeddings = OpenAIEmbeddings()
llm= ChatOpenAI(temperature=0,model='gpt-3.5-turbo')

from langchain.vectorstores import FAISS
db = FAISS.from_documents(documents, embeddings)

from langchain.memory import ConversationBufferMemory
memory= ConversationBufferMemory(
    llm=llm,
    output_key='answer',
    memory_key='chat_history',
    return_messages=True)

retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={"k":5,"include_metadata": True})

prompt_template = """

Answer the questions like a tour guide would in a systematic way.
and do not answer to the question that you do not know.
Context: \n{context}?\n
question: \n{question} \n

Answer:
"""
from langchain.prompts import(
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
messages = [
    SystemMessagePromptTemplate.from_template(prompt_template),
]
qa_prompt=ChatPromptTemplate.from_messages(messages)

chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    memory=memory,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    get_chat_history=lambda h : h,
    combine_docs_chain_kwargs={'prompt': qa_prompt},
    verbose=False)

chat_history=[]

query = "Hi there, how are you?"
result=chain({"question":query,"chat_history": chat_history})
result['answer']

####################################
query = "Make me an itinerary for my trip in goa"
result= chain({"question": query, "chat_history": chat_history})
result['answer']
####################################



!pip install streamlit streamlit_chat
!npm install localtunnel

# Commented out IPython magic to ensure Python compatibility.
# %%writefile APP.py
# 
# import streamlit as st
# import openai
# import os
# # Set up your OpenAI API key
# os.environ['OPENAI_API_KEY'] = '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'
# from langchain.embeddings import OpenAIEmbeddings
# from langchain.chains import LLMChain
# from langchain.chains.question_answering import load_qa_chain
# from langchain.chains import ConversationalRetrievalChain
# from langchain.llms import OpenAI
# from langchain.chat_models import ChatOpenAI
# from langchain.prompts import SystemMessagePromptTemplate
# from langchain import PromptTemplate
# from streamlit_chat import message
# 
# st.set_page_config(page_title="TravelBot", page_icon=":smile:")
# 
# st.title('Welcome to WanderBot!')
# 
# tab1= st.tabs(["ðŸ“ˆ Talk Here"])
# 
# # from langchain.document_loaders import TextLoader  #for textfiles
# # loader = TextLoader('/content/Project 1.pdf')
# # documents = loader.load()
# 
# from langchain.document_loaders import PyPDFLoader
# loader = PyPDFLoader("/content/Project 1.pdf")
# pages = loader.load_and_split()
# 
# # from langchain.document_loaders import TextLoader  #for textfiles
# # loader = TextLoader('/content/Project 2.pdf')
# # documents = loader.load()
# 
# from langchain.document_loaders import PyPDFLoader
# loader = PyPDFLoader("/content/Project 2.pdf")
# pages = loader.load_and_split()
# 
# from langchain.text_splitter import CharacterTextSplitter
# text_splitter = CharacterTextSplitter(chunk_size=800, chunk_overlap=50)
# documents = text_splitter.split_documents(pages)
# 
# embeddings = OpenAIEmbeddings()
# llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')
# 
# from langchain.vectorstores import FAISS
# db = FAISS.from_documents(documents, embeddings)
# 
# from langchain.memory import ConversationBufferMemory
# memory = ConversationBufferMemory(
#     llm=llm,
#     output_key='answer',
#     memory_key='chat_history',
#     return_messages=True)
# 
# retriever = db.as_retriever(
#     search_type="mmr",
#     search_kwargs={"k": 5, "include_metadata": True})
# 
# prompt_template = """
# 
# Answer the questions like a teacher would in a systematic way.
# If the query is outside this domain then do not answer it.
# Context: \n {context}?\n
# question: \n {question} \n
# 
# Answer:
# 
# """
# from langchain.prompts import (
#     ChatPromptTemplate,
#     SystemMessagePromptTemplate,
#     HumanMessagePromptTemplate,
# )
# 
# messages = [
#             SystemMessagePromptTemplate.from_template(prompt_template),
# ]
# 
# qa_prompt = ChatPromptTemplate.from_messages(messages)
# 
# chain = ConversationalRetrievalChain.from_llm(
#   llm=llm,
#   memory=memory,
#   chain_type="stuff",
#   retriever=retriever,
#   return_source_documents=True,
#   get_chat_history=lambda h : h,
#   combine_docs_chain_kwargs={'prompt': qa_prompt},
#   verbose=False)
# 
# if "chat_history" not in st.session_state:
#     st.session_state["chat_history"] = []
# if "generated" not in st.session_state:
#     st.session_state["generated"] = ["Hello! ðŸŒ Welcome to YourTravelBuddy! ðŸŒ´"]
# if "past" not in st.session_state:
#     st.session_state["past"] = ["Hey ! ðŸ‘‹"]
# 
# #container for the chat history
# response_container = st.container()
# #container for the user's text input
# container = st.container()
# 
# def generate_response(query):
#     result = chain({"question": query, "chat_history": st.session_state["chat_history"]})
#     #st.write(result['answer'])
#     st.session_state["chat_history"] = [(query, result["answer"])]
#     with st.sidebar:
#         st.write(result['source_documents'])
#     return result["answer"]
# 
# with container:
#   with st.form(key="my_form", clear_on_submit=True):
#       user_input = st.text_input("You:", key="input")
#       submit_button = st.form_submit_button(label="Send")
# 
#       if user_input and submit_button:
#         output = generate_response(user_input)
#         #print(output)
#         st.session_state["past"].append(user_input)
#         st.session_state["generated"].append(output)
#         st.session_state["chat_history"] = [(user_input, output)]
# 
# with response_container:
#     for i in range(len(st.session_state['generated'])):
#         message(st.session_state["past"][i], is_user=True, key=str(i) + '_user', avatar_style="adventurer")
#         message(st.session_state["generated"][i], key=str(i))

!curl https://ipv4.icanhazip.com/

!streamlit run /content/APP.py &>/content/logs.txt &

!npx localtunnel --port 8501